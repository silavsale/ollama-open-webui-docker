# Run Ollama with models and web ui - in Docker or in Docker - Kubernetes
